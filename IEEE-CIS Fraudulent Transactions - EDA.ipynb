{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "This notebook will go into some brief EDA and a baseline Xgboost model for the transactions and identity data provided by the Kaggle competition, IEEE-CIS Fraudulent Transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import datetime\n",
    "import missingno as msno\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "import gc\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from time import time\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "In this competition we are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n",
    "\n",
    "The data is broken into two files identity and transaction, which are joined by TransactionID.\n",
    "\n",
    "Note: Not all transactions have corresponding identity information.\n",
    "\n",
    "Categorical Features - Transaction\n",
    "\n",
    "- ProductCD: product code, the product for each transaction\n",
    "- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n",
    "- addr1, addr2: billing region, billing country\n",
    "- P_emaildomain: purchaser email domain\n",
    "- R_emaildomain: recipient email domain\n",
    "- M1 - M9: match, such as names on card and address, etc.\n",
    "\n",
    "Categorical Features - Identity\n",
    "Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. \n",
    "They're collected by Vesta’s fraud protection system and digital security partners.\n",
    "- DeviceType: \n",
    "- DeviceInfo\n",
    "- id_12 - id_38\n",
    "\n",
    "Numerical features\n",
    "- TransactionAMT: transaction payment amount in USD. Non-US transactions have an exchange rate applied, so are not exact, have a number of extra dp's. This is potentially already marked in ProductCD as C.\n",
    "- TrasactionDT: timedelta from a given reference datetime (not an actual timestamp). TransactionDT first value is 86400, which corresponds to the number of seconds in a day (60 * 60 * 24 = 86400) so I think the unit is seconds. Using this, we know the data spans 6 months, as the maximum value is 15811131, which would correspond to day 183. *Might be good to split train/validation sets by time, since train/test is split by time\n",
    "- dist: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\n",
    "- C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc.\n",
    "- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
    "- D1-D15: timedelta, such as days between previous transaction, etc.\n",
    "\n",
    "The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp, but we can potentially still use this to build time dependent features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of Fraud\n",
    "Below is the definition of fraud by one of the Vesta team organisers:\n",
    "\"The logic of our labeling is define reported chargeback on the card as fraud transaction (isFraud=1) and transactions posterior to it with either user account, email address or billing address directly linked to these attributes as fraud too. If none of above is reported and found beyond 120 days, then we define as legit transaction (isFraud=0).\n",
    "However, in real world fraudulent activity might not be reported, e.g. cardholder was unaware, or forgot to report in time and beyond the claim period, etc. In such cases, supposed fraud might be labeled as legit, but we never could know of them. Thus, we think they're unusual cases and negligible portion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# identity_train = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\n",
    "# identity_test = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\n",
    "# transaction_train = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\n",
    "# transaction_test = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(identity_train.shape)\n",
    "# print(identity_test.shape)\n",
    "# print(transaction_train.shape)\n",
    "# print(transaction_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transactionID is a unique key throughout the datasets. There are 590k transactions, all unique. In terms of identity, we only have identity data for 144233 out of those 590k transactions (~24%)\n",
    "\n",
    "The 'isFraud' flag is the target variable. As expected, a heavy imbalance, with about 96.5% of non-fraud transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction_train['isFraud'].value_counts(normalize=True).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(transaction_train['TransactionID'].nunique())\n",
    "# print(identity_train['TransactionID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del transaction_test,identity_test, transaction_train, identity_train\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the train and test sets from my earlier pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.read_pickle('/kaggle/input/ieee-cis-fraudulent-transactions-data-prep/train_full.pkl')\n",
    "test_full = pd.read_pickle('/kaggle/input/ieee-cis-fraudulent-transactions-data-prep/test_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of uniques for all variables in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, values in train_full.iteritems():\n",
    "    num_uniques = values.nunique()\n",
    "    print ('{name}: {num_unique}'.format(name=col, num_unique=num_uniques))\n",
    "    print (values.unique())\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missingness Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 999)\n",
    "def missing_data(df) :\n",
    "    count = df.isnull().sum()\n",
    "    percent = (df.isnull().sum()) / (df.isnull().count()) * 100\n",
    "    total = pd.concat([count, percent], axis=1, keys = ['Count', 'Percent'])\n",
    "    types = []\n",
    "    for col in df.columns :\n",
    "        dtypes = str(df[col].dtype)\n",
    "        types.append(dtypes)\n",
    "    total['dtypes'] = types\n",
    "    \n",
    "    return np.transpose(total)\n",
    "\n",
    "missing_df = missing_data(train_full)\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw missingness heatmap\n",
    "msno.matrix(train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty sparse dataset, with alot of NAs. Let's do some more EDA.\n",
    "\n",
    "### Numerical Variables\n",
    "Let's start with a 5 number summary for numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full[train_full.columns[train_full.columns.str.contains('V')]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features engineering by Vesta are very difficult to decipher and all have very different ranges.\n",
    "\n",
    "Next we'll look at the distributions of dist columns vs Fraud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full[train_full.columns[train_full.columns.str.contains('dist')]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(variable):\n",
    "    sns.distplot(train_full[variable][train_full.isFraud==1], kde=True, hist=False,label=\"fraud\")\n",
    "    sns.distplot(train_full[variable][train_full.isFraud==0], kde=True, hist=False,label=\"notfraud\")\n",
    "    plt.legend(prop={'size': 10}, title = 'IsFraud')\n",
    "    plt.title(variable +' vs IsFraud')\n",
    "\n",
    "plot_dist('dist1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist('dist2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly minor differences in distribution, although it looks like a high portion of fraud where the dist1 is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist('TransactionAmt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely some relationship between transaction amount and fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist('TransactionDT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The peak in notfraud distribution could be caused more by the total transactions increasing during christmas. Anyhow, the date itself won't be a feature due to overfit and leakage problems, but we could probably use a day of week and hour of day feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables C1-C14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full[train_full.columns[train_full.columns.str.contains('C')]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also various numbers given in the C variables. Hard to decipher since it is masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in train_full.columns if 'C' in col or col==\"isFraud\"]\n",
    "\n",
    "(train_full[cols].corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not alot of correlation between C variables and Fraud, but the C variables have alot of correlation with each other. Eg. (C2,C2,C4,C6,C7,C8,C10,C11,C12,C14), (C5,C9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in train_full.columns if 'D' in col or col==\"isFraud\"]\n",
    "\n",
    "(train_full[cols].corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some minor correlations between IsFraud and D variables. Again a number of correlations between the D variables themselves. Most promising are D7 and D8. And below, we see that the total not fraud transactions spike at a low number and also at a random number ~360?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist('D7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist('D8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='ProductCD',hue=\"isFraud\",data=train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.ProductCD,train_full.isFraud, normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largest portion of fraud is happening with ProductCD = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, values in train_full[train_full.columns[train_full.columns.str.contains('card')]].iteritems():\n",
    "    num_uniques = values.nunique()\n",
    "    print ('{name}: {num_unique}'.format(name=col, num_unique=num_uniques))\n",
    "    print (values.unique())\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alot of categories in the card variables. Card 4 and 6 have a much smaller number and recognisable categories. And seen below there are many small card1 catgories which have never had fraud. This could result in some overfitting or noise. We will need a way to treat the rare categories. Maybe group them together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.card1,train_full.isFraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.card4,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More fraud happens in discover, but thats not a very large category. FOr mastercard and visa, theres very little difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.groupby('card1').size().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.groupby('card2').size().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.groupby('card3').size().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.groupby('card5').size().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alot of rare categories. It would be good to take only top 20 categories and group the rest in \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.card6,train_full.isFraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full.groupby('card6').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not many cards in the charge card or debit or credit category. These don't have Fraud example either. This could be bad if there are any of these in the test set. In the test set we only have 1 charge card and none in 'debit or credit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.groupby('addr1').size().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.groupby('addr2').size().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.groupby(['addr2', 'addr1']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large number of billing regions, less for billing country. Still, alot of rare categories in both. The biggest country is likely the US. There seems to be only 437 unique combos of region+country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.groupby('P_emaildomain').size().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.P_emaildomain,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.groupby('R_emaildomain').size().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.R_emaildomain,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The email domain as expected are mostly gmail, hotmail and yahoo. The most fraud happens on Gmail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, values in train_full[train_full.columns[train_full.columns.str.contains('M')]].iteritems():\n",
    "    num_uniques = values.nunique()\n",
    "    print ('{name}: {num_unique}'.format(name=col, num_unique=num_uniques))\n",
    "    print (values.unique())\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a True/False matching check. Perhaps if there is a number of failed matching checks this could flag as fraud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.M1,train_full.isFraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.M2,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.M3,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.M4,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.M5,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.M6,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.M7,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.M8,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.M9,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a heap of correlation between each the M variables with the Fraud. Might be useful to look at multiple M variables with Fraud. Also M4 seems to be a combination of M0,M1 and M2. M4==M2 seems to have a much higher rate of Fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.DeviceType,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More fraud happens on mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.groupby('DeviceInfo').size()[:20].sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 Device Infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, values in train_full[train_full.columns[train_full.columns.str.contains('id')]].iteritems():\n",
    "    num_uniques = values.nunique()\n",
    "    print ('{name}: {num_unique}'.format(name=col, num_unique=num_uniques))\n",
    "    print (values.unique())\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_30: Operating system; id_31: browser; id_23: Ip proxy info; id_33: Screen resolution; id_35-38: True/false; id_27-29: Found/notfound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.id_23,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.id_35,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.id_36,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.id_37,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.id_38,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.id_27,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.id_28,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_full.id_29,train_full.isFraud,normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymous proxy is much more likely to have fraud than the other proxy ips, but theres not that many proxy ips to start with. Probably the fact that they have a proxy ip is a good feature?\n",
    "id_35-38 seem to have good correlation too. False has more chance of fraud. id_27-29: clearly found is more likely to be Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "- build time of day/ week, month features (D9 is already a time of day feature, but because it has alot of NAs, better to create using the timedelta variable)\n",
    "- Hour and TransactionPerHour\n",
    "- Number of failed matching checks\n",
    "- proxy ip or not\n",
    "- create category for NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Time dependent features\n",
    "# # https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\n",
    "# train_full['Transaction_day_of_week'] = np.floor((train_full['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "# test_full['Transaction_day_of_week'] = np.floor((test_full['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "# train_full['Transaction_hour'] = np.floor(train_full['TransactionDT'] / 3600) % 24\n",
    "# test_full['Transaction_hour'] = np.floor(test_full['TransactionDT'] / 3600) % 24"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
