{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import datetime\n",
    "import missingno as msno\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, train_test_split,StratifiedKFold\n",
    "import gc\n",
    "from statistics import mean \n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.read_pickle('data/train_full.pkl')\n",
    "test_full = pd.read_pickle('data/test_full.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 434)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = train_full.copy()\n",
    "test_feat = test_full.copy()\n",
    "\n",
    "REMOVE_COLS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat['Transaction_day_of_week'] = np.floor((train_full['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "test_feat['Transaction_day_of_week'] = np.floor((test_full['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "train_feat['Transaction_hour'] = np.floor(train_full['TransactionDT'] / 3600) % 24\n",
    "test_feat['Transaction_hour'] = np.floor(test_full['TransactionDT'] / 3600) % 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decimal part of the transaction amount.\n",
    "train_feat['TransactionAmt_decimal'] = ((train_full['TransactionAmt'] - train_full['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "test_feat['TransactionAmt_decimal'] = ((test_full['TransactionAmt'] - test_full['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "\n",
    "# Whether or not decimal in transaction amount\n",
    "train_feat['TransactionAmt_decimalTF'] = np.where(train_feat['TransactionAmt_decimal']!=0,1,0)\n",
    "test_feat['TransactionAmt_decimalTF'] = np.where(test_feat['TransactionAmt_decimal']!=0,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "dates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n",
    "us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n",
    "\n",
    "for df in [train_feat, test_feat]:\n",
    "    # Temporary variables for aggregation\n",
    "    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "    df['DT_M'] = ((df['DT'].dt.year-2017)*12 + df['DT'].dt.month).astype(np.int8)\n",
    "    df['DT_W'] = ((df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n",
    "    df['DT_D'] = ((df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear).astype(np.int16)\n",
    "    df['DT_hour'] = (df['DT'].dt.hour).astype(np.int8)\n",
    "    # Holidays\n",
    "    df['is_holiday'] = (df['DT'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n",
    "\n",
    "for col in ['DT_M','DT_W','DT_D', 'DT_hour']:\n",
    "    temp_df = pd.concat([train_feat[[col]], test_feat[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()\n",
    "            \n",
    "    train_feat[col+'_total'] = train_feat[col].map(fq_encode)\n",
    "    test_feat[col+'_total']  = test_feat[col].map(fq_encode)\n",
    "    \n",
    "    # We can't use it as solo feature\n",
    "    REMOVE_COLS.append(col+'_total')\n",
    "    \n",
    "# Remove temporary features from final list\n",
    "REMOVE_COLS += ['DT','DT_M','DT_W','DT_D','DT_hour']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try frequency encoding by timeblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeblock_frequency_encoding(train_df, test_df, periods, columns, \n",
    "                                 with_proportions=True, only_proportions=False):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            train_df[new_col] = train_df[col].astype(str)+'_'+train_df[period].astype(str)\n",
    "            test_df[new_col]  = test_df[col].astype(str)+'_'+test_df[period].astype(str)\n",
    "\n",
    "            temp_df = pd.concat([train_df[[new_col]], test_df[[new_col]]])\n",
    "            fq_encode = temp_df[new_col].value_counts().to_dict()\n",
    "\n",
    "            train_df[new_col] = train_df[new_col].map(fq_encode)\n",
    "            test_df[new_col]  = test_df[new_col].map(fq_encode)\n",
    "            \n",
    "            if only_proportions:\n",
    "                train_df[new_col] = train_df[new_col]/train_df[period+'_total']\n",
    "                test_df[new_col]  = test_df[new_col]/test_df[period+'_total']\n",
    "\n",
    "            if with_proportions:\n",
    "                train_df[new_col+'_proportions'] = train_df[new_col]/train_df[period+'_total']\n",
    "                test_df[new_col+'_proportions']  = test_df[new_col]/test_df[period+'_total']\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# time block frequency encoding \n",
    "for df in [train_feat, test_feat]:\n",
    "    df['bank_type'] = df['card3'].astype(str) +'_'+ df['card5'].astype(str)\n",
    "REMOVE_COLS.append('bank_type')\n",
    "periods = ['DT_M','DT_W','DT_D']\n",
    "\n",
    "# Product type\n",
    "train_feat['product_type'] = train_feat['ProductCD'].astype(str)+'_'+train_feat['TransactionAmt'].astype(str)\n",
    "test_feat['product_type'] = test_feat['ProductCD'].astype(str)+'_'+test_feat['TransactionAmt'].astype(str)\n",
    "REMOVE_COLS.append('product_type')\n",
    "i_cols = ['product_type', 'bank_type']\n",
    "periods = ['DT_D','DT_W','DT_M']\n",
    "train_feat, test_feat = timeblock_frequency_encoding(train_feat, test_feat, periods, i_cols, \n",
    "                                                 with_proportions=False, only_proportions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Card1-Card6\n",
    "- frequency encoding\n",
    "- lets try group the rare cards in the 2nd round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['card1','card2','card3','card4','card5','card6']:\n",
    "    train_feat[col]=train_feat[col].astype(object)\n",
    "    test_feat[col]=test_feat[col].astype(object)\n",
    "    \n",
    "    temp_df = pd.concat([train_feat[[col]], test_feat[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n",
    "    train_feat[col+'_freq'] = train_feat[col].map(fq_encode)\n",
    "    test_feat[col+'_freq']  = test_feat[col].map(fq_encode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card1_freq</th>\n",
       "      <th>card2_freq</th>\n",
       "      <th>card3_freq</th>\n",
       "      <th>card4_freq</th>\n",
       "      <th>card5_freq</th>\n",
       "      <th>card6_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>17587</td>\n",
       "      <td>956845</td>\n",
       "      <td>9524</td>\n",
       "      <td>309</td>\n",
       "      <td>267648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1338</td>\n",
       "      <td>5593</td>\n",
       "      <td>956845</td>\n",
       "      <td>347386</td>\n",
       "      <td>49491</td>\n",
       "      <td>267648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1794</td>\n",
       "      <td>70496</td>\n",
       "      <td>956845</td>\n",
       "      <td>719649</td>\n",
       "      <td>102930</td>\n",
       "      <td>824959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7635</td>\n",
       "      <td>11287</td>\n",
       "      <td>956845</td>\n",
       "      <td>347386</td>\n",
       "      <td>47061</td>\n",
       "      <td>824959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>27225</td>\n",
       "      <td>956845</td>\n",
       "      <td>347386</td>\n",
       "      <td>49491</td>\n",
       "      <td>267648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   card1_freq  card2_freq  card3_freq  card4_freq  card5_freq  card6_freq\n",
       "0          56       17587      956845        9524         309      267648\n",
       "1        1338        5593      956845      347386       49491      267648\n",
       "2        1794       70496      956845      719649      102930      824959\n",
       "3        7635       11287      956845      347386       47061      824959\n",
       "4          30       27225      956845      347386       49491      267648"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat[['card1_freq','card2_freq','card3_freq','card4_freq','card5_freq','card6_freq']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_feat, test_feat]:\n",
    "    df['card1_card2'] = df['card1'].astype(str)+ '_' + df['card2'].astype(str)\n",
    "    df['card4_card6'] = df['card4'].astype(str)+ '_' + df['card6'].astype(str)\n",
    "    \n",
    "for col in ['card1_card2','card4_card6']:\n",
    "    temp_df = pd.concat([train_feat[[col]], test_feat[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n",
    "    train_feat[col+'_freq'] = train_feat[col].map(fq_encode)\n",
    "    test_feat[col+'_freq']  = test_feat[col].map(fq_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cols = ['card1','card2','card3','card5']\n",
    "\n",
    "for col in i_cols:\n",
    "    for agg_type in ['mean','std']:\n",
    "        new_col_name = col+'_TransactionAmt_'+agg_type\n",
    "        temp_df = pd.concat([train_feat[[col, 'TransactionAmt']], test_feat[[col,'TransactionAmt']]])\n",
    "        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n",
    "                                                columns={agg_type: new_col_name})\n",
    "        \n",
    "        temp_df.index = list(temp_df[col])\n",
    "        temp_df = temp_df[new_col_name].to_dict()   \n",
    "    \n",
    "        train_feat[new_col_name] = train_feat[col].map(temp_df)\n",
    "        test_feat[new_col_name]  = test_feat[col].map(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addresses\n",
    "- combine addr1 and addr2\n",
    "- frequency encode the combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_feat, test_feat]:\n",
    "    df['addr1_addr2'] =  df['addr1'].astype(str)+ '_' + df['addr2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat([train_feat[['addr1_addr2']], test_feat[['addr1_addr2']]])\n",
    "fq_encode = temp_df['addr1_addr2'].value_counts(dropna=False).to_dict()   \n",
    "train_feat['addr1_addr2_freq'] = train_feat['addr1_addr2'].map(fq_encode)\n",
    "test_feat['addr1_addr2_freq']  = test_feat['addr1_addr2'].map(fq_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distances\n",
    "- multiplication of dist1*dist2\n",
    "- addition of dist1+dist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_feat, test_feat]:\n",
    "    df['dist1_x_dist2'] = df['dist1'] * df['dist2']\n",
    "    df['dist1_plus_dist2'] = df['dist1'] + df['dist2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email Domains\n",
    "- group up gmail, hotmail, yahoo, anonymous, live, outlook, aol etc\n",
    "- group up suffixes with .net and .com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in [train_feat, test_feat]:\n",
    "#     m1 = df.P_emaildomain.str.contains('gmail',na=False)\n",
    "#     m2 = df.P_emaildomain.str.contains('hotmail',na=False)\n",
    "#     m3 = df.P_emaildomain.str.contains('yahoo',na=False)\n",
    "#     m4 = df.P_emaildomain.str.contains('anonymous',na=False)\n",
    "#     m5 = df.P_emaildomain.str.contains('live',na=False)\n",
    "#     m6 = df.P_emaildomain.str.contains('outlook',na=False)\n",
    "#     m7= df.P_emaildomain.str.contains('aol',na=False)\n",
    "#     m8= df.P_emaildomain.str.contains('msn',na=False)\n",
    "#     m9=df.P_emaildomain.str.contains('icloud',na=False)\n",
    "#     m10 =df.P_emaildomain.str.contains('comcast',na=False)\n",
    "\n",
    "#     df['P_emaildomain_knowngroups'] = np.select([m1,m2,m3,m4,m5,m6,m7,m8,m9,m10], ['gmail','hotmail','yahoo','anon','live','outlook','aol','msn','icloud','comcast'], default='other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in [train_feat, test_feat]:\n",
    "#     m1 = df.R_emaildomain.str.contains('gmail',na=False)\n",
    "#     m2 = df.R_emaildomain.str.contains('hotmail',na=False)\n",
    "#     m3 = df.R_emaildomain.str.contains('yahoo',na=False)\n",
    "#     m4 = df.R_emaildomain.str.contains('anonymous',na=False)\n",
    "#     m5 = df.R_emaildomain.str.contains('live',na=False)\n",
    "#     m6 = df.R_emaildomain.str.contains('outlook',na=False)\n",
    "#     m7= df.R_emaildomain.str.contains('aol',na=False)\n",
    "#     m8= df.R_emaildomain.str.contains('msn',na=False)\n",
    "#     m9= df.R_emaildomain.str.contains('icloud',na=False)\n",
    "#     m10 =df.R_emaildomain.str.contains('comcast',na=False)\n",
    "\n",
    "#     df['R_emaildomain_knowngroups'] = np.select([m1,m2,m3,m4,m5,m6,m7,m8,m9,m10], ['gmail','hotmail','yahoo','anon','live','outlook','aol','msn','icloud','comcast'], default='other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in [train_feat, test_feat]:\n",
    "#     m1 = df.P_emaildomain.str.contains('net',na=False)\n",
    "#     m2 = df.P_emaildomain.str.contains('com',na=False)\n",
    "\n",
    "#     df['P_emaildomain_net_com'] = np.select([m1,m2], ['net','com'], default='other')\n",
    "\n",
    "# for df in [train_feat, test_feat]:\n",
    "#     m1 = df.R_emaildomain.str.contains('net',na=False)\n",
    "#     m2 = df.R_emaildomain.str.contains('com',na=False)\n",
    "\n",
    "#     df['R_emaildomain_net_com'] = np.select([m1,m2], ['net','com'], default='other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train_feat[c + '_bin'] = train_feat[c].map(emails)\n",
    "    test_feat[c + '_bin'] = test_feat[c].map(emails)\n",
    "    \n",
    "    train_feat[c + '_suffix'] = train_feat[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test_feat[c + '_suffix'] = test_feat[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train_feat[c + '_suffix'] = train_feat[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test_feat[c + '_suffix'] = test_feat[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M Features\n",
    "- count of M features == true\n",
    "- convert T and F categories to numeric\n",
    "- count number of NA in the M features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n",
    "for c in M_cols:\n",
    "    if train_feat[c].dtype == 'O':\n",
    "        train_feat[c] = train_feat[c].map({\"T\": 1.0, \"F\": 0.0})\n",
    "    if test_feat[c].dtype == 'O':\n",
    "        test_feat[c] = test_feat[c].map({\"T\": 1.0, \"F\": 0.0})\n",
    "\n",
    "# Sum each M together except for M4 which is catgeorical\n",
    "for df in [train_feat, test_feat]:\n",
    "    df['M_sum'] = df[M_cols].sum(axis=1).astype(np.int8)\n",
    "    df['M_na'] = df[M_cols].isna().sum(axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDs\n",
    "- all ID's are categorical\n",
    "- ID 14 is a timezone. Encode as categorical\n",
    "- proxy vs not proxy (nan)\n",
    "- split OS, browser and screen resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_feat, test_feat]:\n",
    "    df['id_proxy'] = np.where(df['id_23'].isin(['IP_PROXY:TRANSPARENT', 'IP_PROXY:ANONYMOUS', 'IP_PROXY:HIDDEN']),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_feat, test_feat]:\n",
    "    df['id_OS'] = df['id_30'].str.split(' ').str[0]\n",
    "    df['id_browser'] = df['id_31'].str.split(' ').str[0]\n",
    "    df['id_screen_width'] = df['id_33'].str.split('x').str[0]\n",
    "    df['id_screen_height'] = df['id_33'].str.split('x').str[1]\n",
    "    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "    df['device_version'] = df['DeviceInfo'].str.split('/', expand=True)[1]\n",
    "    \n",
    "    df['id_34'] = df['id_34'].str.split(':', expand=True)[1]\n",
    "    df['id_23'] = df['id_23'].str.split(':', expand=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_counts = train_feat['id_browser'].value_counts().to_dict()\n",
    "browsers =[]\n",
    "for key, value in browser_counts.items():\n",
    "    if value>300:\n",
    "        browsers.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_feat, test_feat]:\n",
    "    df['id_browser'] = np.where(df['id_browser'].isin(browsers), df['id_browser'], 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_browsers = [\"samsung browser 7.0\",\"opera 53.0\",\"mobile safari 10.0\",\"google search application 49.0\",\n",
    "               \"firefox 60.0\",\"edge 17.0\",\"chrome 69.0\",\"chrome 67.0 for android\",\"chrome 63.0 for android\",\n",
    "               \"chrome 63.0 for ios\",\"chrome 64.0\",\"chrome 64.0 for android\",\n",
    "               \"chrome 64.0 for ios\",\"chrome 65.0\",\"chrome 65.0 for android\",\"chrome 65.0 for ios\",\n",
    "                \"chrome 66.0\",\"chrome 66.0 for android\",\"chrome 66.0 for ios\"]\n",
    "\n",
    "for df in [train_feat, test_feat]:\n",
    "    df['new_browser'] = np.where(df['id_31'].isin(new_browsers),1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C Features\n",
    "- simple rowwise sum of C1-C9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cols = ['C1','C2', 'C3','C4','C5','C6','C7','C8','C9']\n",
    "for df in [train_feat, test_feat]:\n",
    "    df['C_sum'] = df[i_cols].sum(axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target encoding for ProductCD and M4\n",
    "- careful of leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProductCD and M4 Target mean\n",
    "for col in ['ProductCD','M4']:\n",
    "    temp_dict = train_feat.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(\n",
    "                                                        columns={'mean': col+'_target_mean'})\n",
    "    temp_dict.index = temp_dict[col].values\n",
    "    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "\n",
    "    train_feat[col+'_target_mean'] = train_feat[col].map(temp_dict)\n",
    "    test_feat[col+'_target_mean']  = test_feat[col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregations by groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_a = ['TransactionAmt', 'id_02', 'D15']\n",
    "columns_b = ['card1', 'card4', 'addr1']\n",
    "\n",
    "for col_a in columns_a:\n",
    "    for col_b in columns_b:\n",
    "        for df in [train_feat, test_feat]:\n",
    "            df[f'{col_a}_to_mean_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('mean')\n",
    "            df[f'{col_a}_to_std_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('std')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some more features around transaction amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransactionAmt Features\n",
    "\n",
    "# Let's add some kind of client uID based on cardID ad addr columns\n",
    "# The value will be very specific for each client so we need to remove it\n",
    "# from final feature. But we can use it for aggregations.\n",
    "train_feat['uid'] = train_feat['card1'].astype(str)+'_'+train_feat['card2'].astype(str)\n",
    "test_feat['uid'] = test_feat['card1'].astype(str)+'_'+test_feat['card2'].astype(str)\n",
    "\n",
    "train_feat['uid2'] = train_feat['uid'].astype(str)+'_'+train_feat['card3'].astype(str)+'_'+train_feat['card5'].astype(str)\n",
    "test_feat['uid2'] = test_feat['uid'].astype(str)+'_'+test_feat['card3'].astype(str)+'_'+test_feat['card5'].astype(str)\n",
    "\n",
    "train_feat['uid3'] = train_feat['uid2'].astype(str)+'_'+train_feat['addr1'].astype(str)+'_'+train_feat['addr2'].astype(str)\n",
    "test_feat['uid3'] = test_feat['uid2'].astype(str)+'_'+test_feat['addr1'].astype(str)+'_'+test_feat['addr2'].astype(str)\n",
    "\n",
    "# Check if the Transaction Amount is common or not (we can use freq encoding here)\n",
    "# In our dialog with a model we are telling to trust or not to these values   \n",
    "train_feat['TransactionAmt_check'] = np.where(train_feat['TransactionAmt'].isin(test_feat['TransactionAmt']), 1, 0)\n",
    "test_feat['TransactionAmt_check']  = np.where(test_feat['TransactionAmt'].isin(train_feat['TransactionAmt']), 1, 0)\n",
    "\n",
    "# For our model current TransactionAmt is a noise\n",
    "# https://www.kaggle.com/kyakovlev/ieee-check-noise\n",
    "# (even if features importances are telling contrariwise)\n",
    "# There are many unique values and model doesn't generalize well\n",
    "# Lets do some aggregations\n",
    "i_cols = ['card1','card2','card3','card5','uid','uid2','uid3']\n",
    "\n",
    "for col in i_cols:\n",
    "    for agg_type in ['mean','std']:\n",
    "        new_col_name = col+'_TransactionAmt_'+agg_type\n",
    "        temp_df = pd.concat([train_feat[[col, 'TransactionAmt']], test_feat[[col,'TransactionAmt']]])\n",
    "        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n",
    "        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n",
    "                                                columns={agg_type: new_col_name})\n",
    "        \n",
    "        temp_df.index = list(temp_df[col])\n",
    "        temp_df = temp_df[new_col_name].to_dict()   \n",
    "    \n",
    "        train_feat[new_col_name] = train_feat[col].map(temp_df)\n",
    "        test_feat[new_col_name]  = test_feat[col].map(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Frequency Encoding\n",
    "- frequency encode for device info\n",
    "- device type seems to be simply mobile or desktop so leave alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cols = ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "          'P_emaildomain', 'R_emaildomain',\n",
    "          'DeviceInfo', 'id_proxy','id_OS','id_browser','id_screen_width','id_screen_height',\n",
    "          'id_30','id_33','uid','uid2','uid3'\n",
    "         ]\n",
    "\n",
    "for col in i_cols:\n",
    "    temp_df = pd.concat([train_feat[[col]], test_feat[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n",
    "    train_feat[col+'_freq'] = train_feat[col].map(fq_encode)\n",
    "    test_feat[col+'_freq']  = test_feat[col].map(fq_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V Features\n",
    "- rowwise sum of NA counts\n",
    "- rowwise sum (lets try anyways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_cols = list(train_feat.columns[train_feat.columns.str.contains('V')])\n",
    "for df in [train_feat, test_feat]:\n",
    "    df['V_sum'] = df[V_cols].sum(axis=1).astype(np.int8)\n",
    "    df['V_na'] = df[V_cols].isna().sum(axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave Alone Because there's too many (and i don't have enough time)\n",
    "- V features\n",
    "- D features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up\n",
    "- make sure correct column types\n",
    "- do label encoding for the categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_COLS += ['DT','DT_M','DT_W','DT_D','DT_hour','DT_day_week','DT_day_month','uid','uid2','uid3']\n",
    "ALL_FEATURES_train = [col for col in list(train_feat) if col not in REMOVE_COLS]\n",
    "ALL_FEATURES_test = [col for col in list(test_feat) if col not in REMOVE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 550)\n",
      "(506691, 549)\n"
     ]
    }
   ],
   "source": [
    "train_feat=train_feat[ALL_FEATURES_train]\n",
    "print(train_feat.shape)\n",
    "\n",
    "test_feat=test_feat[ALL_FEATURES_test]\n",
    "print(test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars = []\n",
    "for i in range(12,39):\n",
    "    id_vars.append('id_'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the categoricals are correct types\n",
    "for df in [train_feat,test_feat]:\n",
    "    for col in train_feat.columns:\n",
    "        if col in ['card1','card2','card3','card4','card5','card6']:\n",
    "            df[col]=df[col].astype(object)\n",
    "        if col in ['addr1','addr2']:\n",
    "            df[col]=df[col].astype(object)\n",
    "        if col in id_vars:\n",
    "            df[col]=df[col].astype(object)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "#         else:\n",
    "#             df[col] = df[col].astype('category') # try converting objects into categorical and compare performance too\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1158.19 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\martin.cheung\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in less\n",
      "C:\\Users\\martin.cheung\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 967.27 MB\n",
      "Decreased by 16.5%\n",
      "Memory usage of dataframe is 983.83 MB\n",
      "Memory usage after optimization is: 822.92 MB\n",
      "Decreased by 16.4%\n"
     ]
    }
   ],
   "source": [
    "train_feat = reduce_mem_usage(train_feat)\n",
    "test_feat = reduce_mem_usage(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to pickle format\n",
    "train_feat.to_pickle(\"data/train_feat.pkl\")\n",
    "test_feat.to_pickle(\"data/test_feat.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Correlated features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.read_pickle('data/train_feat.pkl')\n",
    "test_full = pd.read_pickle('data/test_feat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-858169713865>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtest_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtest_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unseen_before_label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mlbl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mlbl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mtrain_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlbl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtest_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlbl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \"\"\"\n\u001b[0;32m    219\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36m_encode\u001b[1;34m(values, uniques, encode)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_encode_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36m_encode_numpy\u001b[1;34m(values, uniques, encode)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m# unique sorts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode_check_unknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[0mar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for f in test_full.columns:\n",
    "    if train_full[f].dtype=='object' or test_full[f].dtype=='object': \n",
    "        train_full[f] = train_full[f].fillna('unseen_before_label')\n",
    "        test_full[f]  = test_full[f].fillna('unseen_before_label')\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(train_full[f].values) + list(test_full[f].values))\n",
    "        train_full[f] = lbl.transform(list(train_full[f].values))\n",
    "        test_full[f] = lbl.transform(list(test_full[f].values)) \n",
    "\n",
    "# Fill NA's for numerics\n",
    "train_full = train_full.fillna(-999)\n",
    "test_full = test_full.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of 98% correlated features\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric_df = train_full.select_dtypes(include=numerics)\n",
    "\n",
    "# calculate the correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.98\n",
    "correlated_features = [column for column in upper.columns if any(upper[column] > 0.98)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "with open('data/corr_feat.pkl', 'wb') as f:\n",
    "    pickle.dump(correlated_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
