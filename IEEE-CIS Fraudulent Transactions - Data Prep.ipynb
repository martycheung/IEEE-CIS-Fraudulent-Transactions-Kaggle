{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Data Prep\nThis notebook will not do much analysis, only some basic data prep and saving into pickle format, so I don't waste time trying to re-process the data every time I want to do any analysis or model building."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport datetime\nimport missingno as msno\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn import preprocessing\nimport gc\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nfrom time import time\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Data\nIn this competition we are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n\nThe data is broken into two files identity and transaction, which are joined by TransactionID.\n\nNote: Not all transactions have corresponding identity information.\n\nCategorical Features - Transaction\n\n- ProductCD: product code, the product for each transaction\n- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n- addr1, addr2\n- P_emaildomain: purchaser email domain\n- R_emaildomain: recipient email domain\n- M1 - M9: match, such as names on card and address, etc.\n\nCategorical Features - Identity\nVariables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. \nThey're collected by Vesta’s fraud protection system and digital security partners.\n- DeviceType: \n- DeviceInfo\n- id_12 - id_38\n\nNumerical features\n- TransactionAMT: transaction payment amount in USD. Non-US transactions have an exchange rate applied, so are not exact, have a number of extra dp's. This is potentially already marked in ProductCD as C.\n- TrasactionDT: timedelta from a given reference datetime (not an actual timestamp). TransactionDT first value is 86400, which corresponds to the number of seconds in a day (60 * 60 * 24 = 86400) so I think the unit is seconds. Using this, we know the data spans 6 months, as the maximum value is 15811131, which would correspond to day 183. *Might be good to split train/validation sets by time, since train/test is split by time\n- dist: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\n- C1-C14: \n- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n- D1-D15: timedelta, such as days between previous transaction, etc.\n\nThe TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp, but we can potentially still use this to build time dependent features)."},{"metadata":{},"cell_type":"markdown","source":"#### Definition of Fraud\nBelow is the definition of fraud by one of the Vesta team organisers:\n\"The logic of our labeling is define reported chargeback on the card as fraud transaction (isFraud=1) and transactions posterior to it with either user account, email address or billing address directly linked to these attributes as fraud too. If none of above is reported and found beyond 120 days, then we define as legit transaction (isFraud=0).\nHowever, in real world fraudulent activity might not be reported, e.g. cardholder was unaware, or forgot to report in time and beyond the claim period, etc. In such cases, supposed fraud might be labeled as legit, but we never could know of them. Thus, we think they're unusual cases and negligible portion.\""},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"identity_train = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\nidentity_test = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\ntransaction_train = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntransaction_test = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(identity_train.shape)\nprint(identity_test.shape)\nprint(transaction_train.shape)\nprint(transaction_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transaction_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The transactionID is a unique key throughout the datasets. There are 590k transactions, all unique. In terms of identity, we only have identity data for 144233 out of those 590k transactions (~24%)\n\nThe 'isFraud' flag is the target variable. As expected, a heavy imbalance, with about 96.5% of non-fraud transactions."},{"metadata":{"trusted":true},"cell_type":"code","source":"transaction_train['isFraud'].value_counts(normalize=True).to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(transaction_train['TransactionID'].nunique())\nprint(identity_train['TransactionID'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full = pd.merge(transaction_train,identity_train, on = 'TransactionID',how='left')\ntest_full = pd.merge(transaction_test,identity_test, on = 'TransactionID',how='left')\n\ndel transaction_test,identity_test, transaction_train, identity_train\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do Some Processing Step to get into X, y format and encode labels to make processing more efficient. (One hot enocding would create too many sparse columns for an already wide dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\nfor f in train_full.columns:\n    if train_full[f].dtype=='object' or test_full[f].dtype=='object': \n        train_full[f] = train_full[f].fillna('unseen_before_label')\n        test_full[f]  = test_full[f].fillna('unseen_before_label')\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_full[f].values) + list(test_full[f].values))\n        train_full[f] = lbl.transform(list(train_full[f].values))\n        test_full[f] = lbl.transform(list(test_full[f].values))  \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce memory usage of the dataset by converting numeric columns to their minimal types\n# https://www.kaggle.com/c/champs-scalar-coupling/discussion/96655#558801\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ntrain_full = reduce_mem_usage(train_full)\ntest_full = reduce_mem_usage(test_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full.to_pickle(\"train_full.pkl\")\ntest_full.to_pickle(\"test_full.pkl\")\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}